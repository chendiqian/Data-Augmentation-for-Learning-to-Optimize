lr: 1.e-3
weight_decay: 0.
epoch: 1000
patience: 200
batchsize: 128
train_frac: 1.

num_pred_layers: 1