finetune:
  lr: 1.e-3
  weight_decay: 0.
  epoch: 1000
  patience: 200
  batchsize: 128
  train_frac: 0.01
  modelpath: null
  whole: true
  num_pred_layers: 1

pretrain:
  method:
    # this is the only augmentation
    PageRankAugment:
      strength: 0.5

  lr: 1.e-3
  weight_decay: 0.
  epoch: 200
  patience: 50
  batchsize: 256
  num_pred_layers: 1  # this is fixed, see https://github.com/PyGCL/PyGCL/blob/main/examples/MVGRL_graph.py#L127

defaults:
  - defaults